{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Homework 18: Neural Network\n",
    "**Conditions**:\n",
    "- Dataset: fashion_mnist\n",
    "- Metric: accuracy\n",
    "1. Train simple neural network and get accuracy\n",
    "2. Train models using homework conditions and compare results with simple neural network's \n",
    "\n",
    "**Goal**: make experiment on tuning different parameters of neural network\n",
    "\n",
    "**Acceptance criteria**:  NONE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c4fb9af7d7a9866"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 15:48:57.016278: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.regularizers import l1, l2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T13:49:04.535328036Z",
     "start_time": "2023-09-25T13:48:56.160685301Z"
    }
   },
   "id": "4120035b4be772fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating simple neural network to obtain results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28d199fc82e5129a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Loading data\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T13:49:05.224095024Z",
     "start_time": "2023-09-25T13:49:04.543046650Z"
    }
   },
   "id": "d8d18b54835e6007"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Normalizing data\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T13:49:05.521291565Z",
     "start_time": "2023-09-25T13:49:05.230452909Z"
    }
   },
   "id": "4dc21c251cb33db2"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:03:38.028981743Z",
     "start_time": "2023-09-24T16:03:37.963660915Z"
    }
   },
   "id": "466ba721f9d1125a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.5051 - accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 40s 22ms/step - loss: 0.3742 - accuracy: 0.8650\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3366 - accuracy: 0.8779\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3129 - accuracy: 0.8845\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.2954 - accuracy: 0.8907\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2817 - accuracy: 0.8957\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2681 - accuracy: 0.9009\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2578 - accuracy: 0.9037\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.2500 - accuracy: 0.9068\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2412 - accuracy: 0.9107\n",
      "313/313 - 2s - loss: 0.3302 - accuracy: 0.8819 - 2s/epoch - 7ms/step\n",
      "1875/1875 - 11s - loss: 0.2256 - accuracy: 0.9171 - 11s/epoch - 6ms/step\n",
      "\n",
      "Test and train accuracy: 0.8819000124931335 0.9170666933059692\n"
     ]
    }
   ],
   "source": [
    "# Compilingt the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "# Evauluating model's accuracy\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:10:16.316261511Z",
     "start_time": "2023-09-24T16:03:38.020503735Z"
    }
   },
   "id": "672e343a876b27e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results\n",
    "Test accuracy: 0.8819000124931335 \n",
    "Train accuracy: 0.9170666933059692"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47295afc8bcd98af"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change number of neurons in the hidden layer and compare results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d397d4032a91414"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 1.9632 - accuracy: 0.1944\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 1.6922 - accuracy: 0.2760\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 1.5913 - accuracy: 0.3180\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 1.5527 - accuracy: 0.3335\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 1.5296 - accuracy: 0.3420\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 1.5133 - accuracy: 0.3431\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 1.5020 - accuracy: 0.3472\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 1.4930 - accuracy: 0.3463\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 1.4850 - accuracy: 0.3492\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 1.4785 - accuracy: 0.3534\n",
      "313/313 - 1s - loss: 1.4821 - accuracy: 0.3674 - 1s/epoch - 4ms/step\n",
      "1875/1875 - 8s - loss: 1.4770 - accuracy: 0.3658 - 8s/epoch - 4ms/step\n",
      "\n",
      "Test and train accuracy: 0.36739999055862427 0.3657500147819519\n",
      "Quantity of neurons: 1\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.5493 - accuracy: 0.8102\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.4136 - accuracy: 0.8544\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3795 - accuracy: 0.8669\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.3615 - accuracy: 0.8712\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3448 - accuracy: 0.8769\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.3342 - accuracy: 0.8796\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3212 - accuracy: 0.8835\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3131 - accuracy: 0.8875\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3052 - accuracy: 0.8902\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.2992 - accuracy: 0.8909\n",
      "313/313 - 2s - loss: 0.3625 - accuracy: 0.8707 - 2s/epoch - 6ms/step\n",
      "1875/1875 - 12s - loss: 0.2794 - accuracy: 0.8975 - 12s/epoch - 6ms/step\n",
      "\n",
      "Test and train accuracy: 0.8707000017166138 0.8974999785423279\n",
      "Quantity of neurons: 32\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.5209 - accuracy: 0.8190\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.3937 - accuracy: 0.8595\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3569 - accuracy: 0.8709\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3332 - accuracy: 0.8784\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.3178 - accuracy: 0.8845\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3016 - accuracy: 0.8885\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2887 - accuracy: 0.8950\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2790 - accuracy: 0.8971\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.2719 - accuracy: 0.8996\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2643 - accuracy: 0.9017\n",
      "313/313 - 2s - loss: 0.3430 - accuracy: 0.8787 - 2s/epoch - 7ms/step\n",
      "1875/1875 - 11s - loss: 0.2481 - accuracy: 0.9097 - 11s/epoch - 6ms/step\n",
      "\n",
      "Test and train accuracy: 0.8787000179290771 0.9096999764442444\n",
      "Quantity of neurons: 64\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.4823 - accuracy: 0.8293\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3654 - accuracy: 0.8666\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3249 - accuracy: 0.8813\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3003 - accuracy: 0.8880\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2834 - accuracy: 0.8949\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.2690 - accuracy: 0.9006\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2547 - accuracy: 0.9054\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2446 - accuracy: 0.9086\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2356 - accuracy: 0.9123\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2236 - accuracy: 0.9161\n",
      "313/313 - 1s - loss: 0.3255 - accuracy: 0.8914 - 1s/epoch - 5ms/step\n",
      "1875/1875 - 10s - loss: 0.2034 - accuracy: 0.9237 - 10s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8913999795913696 0.9236833453178406\n",
      "Quantity of neurons: 256\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 46s 24ms/step - loss: 0.4793 - accuracy: 0.8300\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.3581 - accuracy: 0.8688\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.3229 - accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.3004 - accuracy: 0.8896\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.2794 - accuracy: 0.8968\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.2667 - accuracy: 0.8998\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.2527 - accuracy: 0.9044\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.2414 - accuracy: 0.9091\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.2326 - accuracy: 0.9128\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 44s 23ms/step - loss: 0.2229 - accuracy: 0.9166\n",
      "313/313 - 2s - loss: 0.3293 - accuracy: 0.8874 - 2s/epoch - 5ms/step\n",
      "1875/1875 - 10s - loss: 0.2033 - accuracy: 0.9240 - 10s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8873999714851379 0.9239833354949951\n",
      "Quantity of neurons: 512\n"
     ]
    }
   ],
   "source": [
    "neurons_qty = [1, 32, 64, 256, 512]\n",
    "\n",
    "for neurons in neurons_qty:\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(neurons, activation='relu'),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "    print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "    print('Quantity of neurons:', neurons)\n",
    "    print()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:36:50.474334560Z",
     "start_time": "2023-09-24T16:10:16.272138227Z"
    }
   },
   "id": "e6a94bf4c85f864d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results (128 neurons)\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Quantity of neurons: 1\n",
    "Test accuracy: 0.36739999055862427 \n",
    "Train accuracy: 0.3657500147819519\n",
    "\n",
    "Quantity of neurons: 32\n",
    "Test accuracy: 0.8707000017166138\n",
    "Train accuracy: 0.8974999785423279\n",
    "\n",
    "Quantity of neurons: 64\n",
    "Test accuracy: 0.8787000179290771\n",
    "Train accuracy: 0.90969997644424447\n",
    "\n",
    "Quantity of neurons: 256\n",
    "Test accuracy: 0.8913999795913696\n",
    "Train accuracy: 0.9236833453178406\n",
    "\n",
    "Quantity of neurons: 512\n",
    "Test accuracy: 0.8873999714851379\n",
    "Train accuracy: 0.9239833354949951"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "319199817976eba3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best train results: 0.9239833354949951 (512 neurons)\n",
    "Best test results: 0.8913999795913696 (256 neurons)\n",
    "\n",
    "Worse train and test results: 1 neuron\n",
    "\n",
    "Best difference between test and train results: 0.0016499757766723633 (1 neuron)\n",
    "Worse difference between test and train results: 0.03658336400985718 (512 neurons)\n",
    "\n",
    "**Conclusions**\n",
    "- Increasing number of neurons gives better results on training set.\n",
    "But in the same time it can cause overfitting.\n",
    "- Low number of neurons is a reason of underfitting.\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "\n",
    "Adjusting the number of neurons in a neural network layer can affect the network's capacity and its ability to learn complex patterns in the data. Here's how the number of neurons can influence the model:\n",
    "\n",
    "1. **Underfitting vs. Overfitting**:\n",
    "   \n",
    "   - **Too Few Neurons**: If you have too few neurons in your network, it may not have enough capacity to capture the underlying patterns in the data. This can lead to underfitting, where the model cannot learn the training data well and performs poorly on both the training and validation sets.\n",
    "\n",
    "   - **Too Many Neurons**: On the other hand, if you have too many neurons, the network may have excessive capacity and can memorize the training data, leading to overfitting. Overfit models perform very well on the training data but generalize poorly to unseen data.\n",
    "\n",
    "2. **Training Time**:\n",
    "   \n",
    "   - **More Neurons**: Increasing the number of neurons typically leads to longer training times. A network with more neurons requires more computations and may require more data to train effectively.\n",
    "\n",
    "3. **Complexity of Learned Features**:\n",
    "   \n",
    "   - **More Neurons**: A network with more neurons can potentially learn more complex and fine-grained features from the data, which can be beneficial for tasks with intricate patterns.\n",
    "\n",
    "   - **Fewer Neurons**: Conversely, a network with fewer neurons may generalize better when the dataset is small or when simpler features are sufficient for the task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77fbd07e9ed773eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try to add additional hidden layer and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fd2574d5beb3d64"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 43s 22ms/step - loss: 0.4865 - accuracy: 0.8251\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.3638 - accuracy: 0.8649\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.3274 - accuracy: 0.8786\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 46s 25ms/step - loss: 0.3063 - accuracy: 0.8868\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.2871 - accuracy: 0.8925\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.2728 - accuracy: 0.8980\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2588 - accuracy: 0.9024\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2488 - accuracy: 0.9063\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.2377 - accuracy: 0.9100\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.2287 - accuracy: 0.9136\n",
      "313/313 - 2s - loss: 0.3523 - accuracy: 0.8788 - 2s/epoch - 5ms/step\n",
      "1875/1875 - 9s - loss: 0.2270 - accuracy: 0.9136 - 9s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8787999749183655 0.9135666489601135\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:44:27.111275715Z",
     "start_time": "2023-09-24T16:36:50.453283359Z"
    }
   },
   "id": "c8695c1f1f4c3bf3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results (128 neurons)\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Additional hidden layer:\n",
    "Test accuracy: 0.8787999749183655\n",
    "Train accuracy: 0.9135666489601135\n",
    "\n",
    "**Conclusion**\n",
    "Additional hidden layer gave lower accuracy and less stable results\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "\n",
    "The number of hidden layers determines the depth of the network, and it's often referred to as the network's depth or depth of the architecture. Here's how the number of hidden layers can influence the results:\n",
    "\n",
    "1. **Model Capacity**:\n",
    "   \n",
    "   - **More Hidden Layers**: Increasing the number of hidden layers increases the model's capacity to learn complex patterns in the data. Deeper networks can potentially capture hierarchical and abstract features, which can be beneficial for tasks with intricate patterns.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Conversely, having fewer hidden layers may limit the model's capacity, and it might struggle to represent complex relationships in the data.\n",
    "\n",
    "2. **Overfitting vs. Underfitting**:\n",
    "   \n",
    "   - **More Hidden Layers**: Deeper networks with more hidden layers are more prone to overfitting, especially when dealing with small datasets. They have a greater potential to memorize the training data, leading to poor generalization.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Simpler models with fewer hidden layers are less likely to overfit, making them more suitable for small datasets or situations where a smaller model suffices.\n",
    "\n",
    "3. **Training Time**:\n",
    "   \n",
    "   - **More Hidden Layers**: Deeper networks typically require longer training times because they involve more computations. Training deep models can also be computationally intensive.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Simpler models with fewer layers tend to train faster, which can be advantageous when you have limited computational resources.\n",
    "\n",
    "4. **Data Requirements**:\n",
    "   \n",
    "   - **More Hidden Layers**: Deeper networks often require more data to train effectively. Having a large dataset can help prevent overfitting when using deep architectures.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Simpler models with fewer layers may perform better when data is limited.\n",
    "\n",
    "5. **Complexity of Learned Features**:\n",
    "   \n",
    "   - **More Hidden Layers**: Deeper networks have the potential to learn complex and hierarchical features, which can be beneficial for tasks like image recognition, natural language processing, and speech recognition.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Simpler models may be sufficient for tasks where the relationships in the data are relatively straightforward.\n",
    "\n",
    "6. **Regularization**:\n",
    "   \n",
    "   - **More Hidden Layers**: Deeper networks may require stronger regularization techniques (e.g., dropout, L1/L2 regularization) to prevent overfitting.\n",
    "\n",
    "   - **Fewer Hidden Layers**: Simpler models may need less regularization.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e896a50b8bfe893a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change activation function of hidden layer to sigmoid and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc51944b369c6cef"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-25 15:49:23.682492: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 52s 27ms/step - loss: 0.5440 - accuracy: 0.8135\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.3929 - accuracy: 0.8583\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.3547 - accuracy: 0.8717\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.3312 - accuracy: 0.8803\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 45s 24ms/step - loss: 0.3118 - accuracy: 0.8869\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 47s 25ms/step - loss: 0.2964 - accuracy: 0.8916\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2839 - accuracy: 0.8953\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.2722 - accuracy: 0.8996\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.2636 - accuracy: 0.9031\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.2525 - accuracy: 0.9074\n",
      "313/313 - 6s - loss: 0.3230 - accuracy: 0.8819 - 6s/epoch - 19ms/step\n",
      "1875/1875 - 16s - loss: 0.2355 - accuracy: 0.9121 - 16s/epoch - 9ms/step\n",
      "\n",
      "Test and train accuracy: 0.8819000124931335 0.9121333360671997\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='sigmoid'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T13:57:43.054170411Z",
     "start_time": "2023-09-25T13:49:23.655727550Z"
    }
   },
   "id": "d41097981c29c4c6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results (relu activator)\n",
    "Test accuracy: 0.8819000124931335 \n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network with sigmoid activator\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9121333360671997\n",
    "\n",
    "**Conclusion**\n",
    "Relu and sigmoid activators showed almost the same results\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "\n",
    "1. **ReLU (Rectified Linear Unit)**:\n",
    "\n",
    "   - **Advantages**:\n",
    "     - **Non-linearity**: ReLU introduces non-linearity into the model, allowing the network to learn complex relationships in the data.\n",
    "     - **Sparsity**: ReLU activations are sparse because they output zero for negative inputs, which can make the network more efficient and easier to train.\n",
    "     - **Gradient Descent**: The derivative of ReLU is either 0 or 1, making it easier for gradient-based optimization algorithms to converge compared to saturating activation functions like sigmoid.\n",
    "\n",
    "   - **Use Cases**:\n",
    "     - ReLU is commonly used as an activation function in hidden layers of deep neural networks, especially in deep convolutional neural networks (CNNs) used for image processing tasks.\n",
    "     - It is well-suited for problems with large amounts of data and complex patterns.\n",
    "\n",
    "   - **Drawbacks**:\n",
    "     - **Dead Neurons**: ReLU units can sometimes become \"dead\" (outputting zero for all inputs) during training if they encounter large gradients. This issue can be addressed with variants like Leaky ReLU and Parametric ReLU (PReLU).\n",
    "\n",
    "2. **Sigmoid**:\n",
    "\n",
    "   - **Advantages**:\n",
    "     - **Smoothness**: Sigmoid is a smooth, differentiable activation function that produces values between 0 and 1. It can be useful for ensuring smooth transitions in the network's outputs.\n",
    "     - **Use in Output Layer**: Sigmoid is often used in the output layer of binary classification models because it squashes the network's output to the range [0, 1], representing probabilities.\n",
    "\n",
    "   - **Use Cases**:\n",
    "     - Sigmoid is historically used in shallow neural networks and logistic regression models.\n",
    "     - It is suitable for binary classification problems when the output needs to be interpreted as probabilities.\n",
    "\n",
    "   - **Drawbacks**:\n",
    "     - **Vanishing Gradients**: Sigmoid tends to suffer from the vanishing gradient problem, especially in deep networks, where gradients can become very small during backpropagation, making training slow and difficult.\n",
    "     - **Saturated Neurons**: Sigmoid activations saturate when the inputs are too far from zero, causing the gradients to become close to zero. This can lead to slow convergence.\n",
    "\n",
    "In summary, the choice between ReLU and sigmoid activations depends on the specific problem you are solving and the architecture of your neural network:\n",
    "\n",
    "- ReLU is the default choice for hidden layers in most modern neural networks because of its non-linearity, efficiency, and ability to mitigate vanishing gradient problems. Variants like Leaky ReLU and PReLU can be used to address the \"dead neuron\" issue.\n",
    "\n",
    "- Sigmoid is still useful in specific scenarios, such as binary classification problems and shallow networks. However, it is less commonly used in deep learning due to its drawbacks, especially in deep architectures."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2aba88addd8a720"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use RMSprop optimizer instead of Adam and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68229bc5bcce7d25"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.5028 - accuracy: 0.8204\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.3740 - accuracy: 0.8652\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3414 - accuracy: 0.8782\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3214 - accuracy: 0.8850\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.3091 - accuracy: 0.8900\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2985 - accuracy: 0.8956\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2912 - accuracy: 0.8993\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.2813 - accuracy: 0.9015\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.2756 - accuracy: 0.9039\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2686 - accuracy: 0.9053\n",
      "313/313 - 2s - loss: 0.3999 - accuracy: 0.8837 - 2s/epoch - 6ms/step\n",
      "1875/1875 - 12s - loss: 0.2433 - accuracy: 0.9151 - 12s/epoch - 7ms/step\n",
      "\n",
      "Test and train accuracy: 0.8837000131607056 0.915149986743927\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T16:57:31.993193913Z",
     "start_time": "2023-09-24T16:50:46.410219102Z"
    }
   },
   "id": "c1d986240fba40a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results (adam optimizer)\n",
    "Test accuracy: 0.8819000124931335 \n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network with RMSprop optimizer\n",
    "Test accuracy: 0.8837000131607056\n",
    "Train accuracy: 0.915149986743927\n",
    " \n",
    "**Conclusion**\n",
    "Adam and RMSprop optimizers showed almost the same results\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "\n",
    "Adam (short for Adaptive Moment Estimation) and RMSprop (short for Root Mean Square Propagation) are both popular optimization algorithms used in training neural networks. They share some similarities but also have distinct differences in their behavior and impact on training. Let's explore the key differences and impacts of these two optimizers:\n",
    "\n",
    "**Adam Optimizer**:\n",
    "\n",
    "1. **Adaptive Learning Rates**:\n",
    "   \n",
    "   - Adam computes adaptive learning rates for each parameter. It adjusts the learning rates based on the past gradients of each parameter, allowing for faster convergence in some cases.\n",
    "\n",
    "2. **Momentum-like Update**:\n",
    "   \n",
    "   - Adam includes a momentum-like term that helps accelerate convergence, especially in the presence of noisy gradients. It combines the concepts of momentum (like SGD with momentum) and RMSprop.\n",
    "\n",
    "3. **Bias Correction**:\n",
    "   \n",
    "   - Adam includes bias correction terms to account for the fact that the moving averages of past gradients are initialized with zeros. This bias correction helps improve the accuracy of parameter updates, especially in the early stages of training.\n",
    "\n",
    "4. **Hyperparameters**:\n",
    "   \n",
    "   - Adam has hyperparameters such as the learning rate (`lr`), β1 (momentum-like term decay rate), and β2 (decay rate for the moving average of squared gradients). These hyperparameters need to be tuned for optimal performance.\n",
    "\n",
    "**RMSprop Optimizer**:\n",
    "\n",
    "1. **Root Mean Square Gradients**:\n",
    "   \n",
    "   - RMSprop computes a moving average of the squared gradients for each parameter. It uses this moving average to scale the learning rates for each parameter individually.\n",
    "\n",
    "2. **No Momentum Term**:\n",
    "   \n",
    "   - Unlike Adam, RMSprop does not have an explicit momentum term. It relies solely on adaptive learning rates to adjust the step sizes for parameter updates.\n",
    "\n",
    "3. **Hyperparameters**:\n",
    "   \n",
    "   - RMSprop has hyperparameters such as the learning rate (`lr`) and a decay factor (usually denoted as γ or β) for the moving average of squared gradients. These hyperparameters also require tuning.\n",
    "\n",
    "**Differences and Impact**:\n",
    "\n",
    "1. **Convergence Speed**:\n",
    "   \n",
    "   - Adam often converges faster than RMSprop due to its adaptive learning rates and momentum-like term. It can handle noisy or sparse gradients more effectively.\n",
    "\n",
    "2. **Stability**:\n",
    "   \n",
    "   - RMSprop can be more stable and robust in some cases because it doesn't include the momentum-like term, which can sometimes lead to oscillations in parameter updates.\n",
    "\n",
    "3. **Hyperparameter Sensitivity**:\n",
    "   \n",
    "   - Adam may be more sensitive to the choice of hyperparameters, and poorly chosen hyperparameters can lead to erratic behavior during training.\n",
    "\n",
    "4. **Common Use Cases**:\n",
    "   \n",
    "   - Adam is commonly used as a default optimizer for many deep learning applications because of its overall good performance and convergence properties.\n",
    "\n",
    "5. **Experimentation**:\n",
    "   \n",
    "   - The choice between Adam and RMSprop may require experimentation. In some cases, one optimizer may outperform the other depending on the specific problem and architecture.\n",
    "\n",
    "In summary, both Adam and RMSprop are effective optimizers, and the choice between them depends on your specific problem, dataset, and experimentation. Adam often converges faster, while RMSprop can be more stable. Hyperparameter tuning is crucial for optimizing the performance of either optimizer.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "293469ae426c8e3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Change number of epochs to 5 and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a058c32e8ac3179"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.4982 - accuracy: 0.8261\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3761 - accuracy: 0.8645\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3351 - accuracy: 0.8758\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3108 - accuracy: 0.8864\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2923 - accuracy: 0.8921\n",
      "313/313 - 2s - loss: 0.3684 - accuracy: 0.8697 - 2s/epoch - 5ms/step\n",
      "1875/1875 - 9s - loss: 0.2851 - accuracy: 0.8951 - 9s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.869700014591217 0.8951333165168762\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:01:08.356521092Z",
     "start_time": "2023-09-24T16:57:31.966839455Z"
    }
   },
   "id": "50bd69afca0910e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results (10 epochs)\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network with 5 epochs\n",
    "Test accuracy: 0.869700014591217 \n",
    "Train accuracy: 0.8951333165168762\n",
    "\n",
    "**Conclusion**\n",
    "Accorucy got lower after number of epoches was redused from 10 to 5 => *underfitting*\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "\n",
    "1. **Underfitting vs. Overfitting**:\n",
    "\n",
    "   - **Fewer Epochs**:\n",
    "     - If you use too few epochs, the model may not have enough training cycles to learn the underlying patterns in the data. This can result in underfitting, where the model's performance is suboptimal both on the training and validation datasets.\n",
    "   \n",
    "   - **More Epochs**:\n",
    "     - Using more epochs allows the model to continue learning from the data. It can help reduce underfitting and improve the model's performance on the training dataset.\n",
    "\n",
    "   - **Risk of Overfitting**:\n",
    "     - However, increasing the number of epochs beyond a certain point can lead to overfitting. Overfitting occurs when the model starts memorizing the training data instead of generalizing from it. This can lead to poor performance on unseen data (validation or test sets).\n",
    "\n",
    "2. **Validation Loss and Accuracy**:\n",
    "\n",
    "   - **Early Stopping**:\n",
    "     - Monitoring the validation loss and accuracy during training is essential. If you notice that the validation loss starts to increase or the validation accuracy plateaus or decreases while the training loss continues to decrease, it may indicate that you've trained for too many epochs, and early stopping might be beneficial.\n",
    "\n",
    "3. **Computational Resources**:\n",
    "\n",
    "   - **Training Time**:\n",
    "     - Using a large number of epochs can increase the training time, especially for deep and complex models. It's important to strike a balance between the number of epochs and available computational resources.\n",
    "\n",
    "4. **Dataset Size**:\n",
    "\n",
    "   - **Impact on Small Datasets**:\n",
    "     - The impact of the number of epochs is more pronounced on smaller datasets. Small datasets tend to overfit more easily, so using an appropriate number of epochs is crucial.\n",
    "\n",
    "5. **Learning Rate Scheduling**:\n",
    "\n",
    "   - **Learning Rate Adaptation**:\n",
    "     - Some training scenarios involve using learning rate scheduling or dynamic learning rates. In these cases, the number of epochs can be related to how the learning rate is adjusted during training.\n",
    "\n",
    "6. **Hyperparameter Tuning**:\n",
    "\n",
    "   - **Hyperparameter Interaction**:\n",
    "     - The number of epochs can interact with other hyperparameters, such as the learning rate, batch size, and regularization strength. Fine-tuning these hyperparameters may require adjusting the number of epochs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6c9d56977ef6c47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use dropout to avoid overfitting and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd0f21a98a5b346e"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.6278 - accuracy: 0.7770\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.4773 - accuracy: 0.8287\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.4445 - accuracy: 0.8394\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.4279 - accuracy: 0.8448\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.4118 - accuracy: 0.8508\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.4017 - accuracy: 0.8539\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3935 - accuracy: 0.8559\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3866 - accuracy: 0.8575\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.3777 - accuracy: 0.8607\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.3690 - accuracy: 0.8635\n",
      "313/313 - 2s - loss: 0.3714 - accuracy: 0.8719 - 2s/epoch - 8ms/step\n",
      "1875/1875 - 10s - loss: 0.2990 - accuracy: 0.8901 - 10s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8719000220298767 0.8901333212852478\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:07:45.136670044Z",
     "start_time": "2023-09-24T17:01:08.361661384Z"
    }
   },
   "id": "b3967c7d7cff5de0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results \n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network using dropout\n",
    "Test accuracy: 0.8719000220298767\n",
    "Train accuracy: 0.8901333212852478\n",
    "\n",
    "**Conclusion**\n",
    "Results are slightly worse but more stable\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "Dropout is a regularization technique commonly used in neural networks to prevent overfitting. It has a significant impact on the training and performance of a model. Dropout works by randomly deactivating (dropping out) a fraction of neurons during each training batch, forcing the model to learn more robust and generalizable features. Here's how dropout can impact your neural network:\n",
    "\n",
    "1. **Reduction in Overfitting**:\n",
    "\n",
    "   - **Impact**: Dropout is primarily used to reduce overfitting. It prevents the network from relying too heavily on specific neurons and encourages it to learn a more diverse set of features.\n",
    "\n",
    "   - **Result**: By reducing overfitting, dropout can lead to improved generalization performance. The model is less likely to memorize the training data and is more likely to perform well on unseen data, such as a validation or test dataset.\n",
    "\n",
    "2. **Regularization**:\n",
    "\n",
    "   - **Impact**: Dropout serves as a form of regularization by adding noise to the network during training. This regularization can help control the complexity of the model and prevent it from fitting the training data too closely.\n",
    "\n",
    "   - **Result**: A model with dropout is less likely to suffer from high variance, which can lead to better convergence during training.\n",
    "\n",
    "3. **Ensemble Effect**:\n",
    "\n",
    "   - **Impact**: Dropout can be seen as training an ensemble of multiple neural networks with shared weights. During each training batch, different neurons are dropped out, effectively training different subnetworks.\n",
    "\n",
    "   - **Result**: The ensemble effect can improve the model's robustness and ability to generalize. It's as if the network has learned to make predictions by considering the opinions of multiple experts (the different subnetworks).\n",
    "\n",
    "4. **Training Time**:\n",
    "\n",
    "   - **Impact**: Dropout can increase training time because it requires multiple forward and backward passes through the network during each epoch. This is because dropout is active during training but not during inference (making predictions).\n",
    "\n",
    "   - **Result**: Longer training time is a trade-off for the benefits of dropout, but it is often a worthwhile trade-off to achieve better generalization.\n",
    "\n",
    "5. **Tuning Dropout Rate**:\n",
    "\n",
    "   - **Impact**: The choice of dropout rate (the fraction of neurons to drop) is a hyperparameter that impacts the model's behavior.\n",
    "\n",
    "   - **Result**: The optimal dropout rate may vary depending on the dataset and network architecture. Too high of a dropout rate can lead to underfitting, while too low of a dropout rate may not provide sufficient regularization.\n",
    "\n",
    "6. **Use in Specific Layers**:\n",
    "\n",
    "   - **Impact**: Dropout is typically applied to hidden layers rather than input or output layers.\n",
    "\n",
    "   - **Result**: This allows the model to maintain more information at the input and output layers while adding regularization and reducing overfitting in the intermediate layers.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4caf216d9513b2a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try different normalizations such as BatchNormalization and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc8b647bb3c910f1"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.4912 - accuracy: 0.8279\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.4052 - accuracy: 0.8560\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.3757 - accuracy: 0.8659\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.3484 - accuracy: 0.8732\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.3359 - accuracy: 0.8781\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.3216 - accuracy: 0.8838\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.3108 - accuracy: 0.8862\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.3045 - accuracy: 0.8887\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2945 - accuracy: 0.8910\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2893 - accuracy: 0.8936\n",
      "313/313 - 1s - loss: 0.3670 - accuracy: 0.8737 - 1s/epoch - 5ms/step\n",
      "1875/1875 - 9s - loss: 0.2692 - accuracy: 0.9032 - 9s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8737000226974487 0.9031500220298767\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:14:27.670099570Z",
     "start_time": "2023-09-24T17:07:45.138007122Z"
    }
   },
   "id": "8f2ceee7846aabb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network using dropout\n",
    "Test accuracy: 0.8737000226974487\n",
    "Train accuracy: 0.9031500220298767\n",
    " \n",
    "**Conclusion**\n",
    "Results are worse\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "Batch Normalization (BatchNorm) is a layer commonly used in neural networks, and it has a significant impact on the training and performance of a model. Here's how Batch Normalization can impact your neural network:\n",
    "\n",
    "1. **Improved Training Speed**:\n",
    "\n",
    "   - **Impact**: BatchNorm can accelerate training by reducing internal covariate shift. It normalizes the activations of a layer, ensuring that they have a consistent mean and variance across mini-batches during training.\n",
    "\n",
    "   - **Result**: Faster convergence is often observed, allowing you to train deeper networks or achieve the same level of performance with fewer training epochs.\n",
    "\n",
    "2. **Reduced Sensitivity to Initialization**:\n",
    "\n",
    "   - **Impact**: BatchNorm makes networks less sensitive to weight initialization. This means you can use higher learning rates without causing instability during training.\n",
    "\n",
    "   - **Result**: Faster training convergence and more robust training dynamics.\n",
    "\n",
    "3. **Mitigation of Vanishing/Exploding Gradient Problems**:\n",
    "\n",
    "   - **Impact**: BatchNorm can help mitigate the vanishing gradient problem by normalizing activations. This is particularly beneficial in deep networks.\n",
    "\n",
    "   - **Result**: Deeper networks can be trained more effectively, leading to better model performance.\n",
    "\n",
    "4. **Regularization Effect**:\n",
    "\n",
    "   - **Impact**: BatchNorm acts as a form of regularization by adding noise to activations. This can help reduce overfitting.\n",
    "\n",
    "   - **Result**: Models with BatchNorm layers tend to be more robust to overfitting, allowing for the training of larger and more complex networks.\n",
    "\n",
    "5. **Smoothing of Loss Landscape**:\n",
    "\n",
    "   - **Impact**: BatchNorm can have a smoothing effect on the loss landscape during training, making optimization more stable.\n",
    "\n",
    "   - **Result**: This helps the model to avoid getting stuck in poor local minima during optimization.\n",
    "\n",
    "6. **Impact on Learning Rate Scheduling**:\n",
    "\n",
    "   - **Impact**: BatchNorm can affect the choice of learning rate scheduling. With BatchNorm, it's often possible to use higher initial learning rates and more aggressive learning rate schedules.\n",
    "\n",
    "   - **Result**: This can lead to faster convergence and better generalization.\n",
    "\n",
    "7. **Use in Convolutional and Fully Connected Layers**:\n",
    "\n",
    "   - **Impact**: BatchNorm can be used in both convolutional and fully connected layers, making it versatile and applicable to a wide range of neural network architectures.\n",
    "\n",
    "   - **Result**: It can improve the training dynamics and generalization of various types of networks, including convolutional neural networks (CNNs) and feedforward neural networks.\n",
    "\n",
    "8. **Impact on Mini-Batch Size**:\n",
    "\n",
    "   - **Impact**: BatchNorm's effectiveness can depend on the mini-batch size used during training. It typically works best with reasonably sized mini-batches.\n",
    "\n",
    "   - **Result**: The choice of mini-batch size should be considered when using BatchNorm."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "397cb84e3737ffc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Try different types of weight initializing (i.e., he_normal or glorot_uniform) and compare results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "696a95444d6cf60c"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "relu_initializers = ['he_normal', 'random_normal', 'random_uniform', 'truncated_normal']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:14:27.673714076Z",
     "start_time": "2023-09-24T17:14:27.643913886Z"
    }
   },
   "id": "cd4e00f99e1a9f3a"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.4996 - accuracy: 0.8226\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.3772 - accuracy: 0.8653\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.3389 - accuracy: 0.8773\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3136 - accuracy: 0.8847\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2961 - accuracy: 0.8899\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.2789 - accuracy: 0.8962\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.2671 - accuracy: 0.9013\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 42s 23ms/step - loss: 0.2582 - accuracy: 0.9042\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.2483 - accuracy: 0.9072\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 0.2398 - accuracy: 0.9100\n",
      "313/313 - 3s - loss: 0.3465 - accuracy: 0.8798 - 3s/epoch - 9ms/step\n",
      "1875/1875 - 8s - loss: 0.2311 - accuracy: 0.9141 - 8s/epoch - 4ms/step\n",
      "\n",
      "Test and train accuracy: 0.879800021648407 0.9140666723251343\n",
      "Initializer: he_normal\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.4998 - accuracy: 0.8246\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3762 - accuracy: 0.8637\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3387 - accuracy: 0.8760\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.3138 - accuracy: 0.8838\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2941 - accuracy: 0.8913\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2809 - accuracy: 0.8958\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2680 - accuracy: 0.8993\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2586 - accuracy: 0.9035\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2468 - accuracy: 0.9070\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2403 - accuracy: 0.9107\n",
      "313/313 - 1s - loss: 0.3533 - accuracy: 0.8780 - 1s/epoch - 4ms/step\n",
      "1875/1875 - 9s - loss: 0.2371 - accuracy: 0.9124 - 9s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.878000020980835 0.9124333262443542\n",
      "Initializer: random_normal\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.5003 - accuracy: 0.8244\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.3790 - accuracy: 0.8635\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.3431 - accuracy: 0.8754\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.3169 - accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.3001 - accuracy: 0.8898\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2835 - accuracy: 0.8950\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.2734 - accuracy: 0.8982\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.2588 - accuracy: 0.9028\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2502 - accuracy: 0.9070\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.2420 - accuracy: 0.9096\n",
      "313/313 - 1s - loss: 0.3421 - accuracy: 0.8782 - 1s/epoch - 4ms/step\n",
      "1875/1875 - 9s - loss: 0.2265 - accuracy: 0.9147 - 9s/epoch - 5ms/step\n",
      "\n",
      "Test and train accuracy: 0.8781999945640564 0.9146999716758728\n",
      "Initializer: random_uniform\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.5049 - accuracy: 0.8221\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3785 - accuracy: 0.8649\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.3386 - accuracy: 0.8764\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.3119 - accuracy: 0.8864\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.2939 - accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.2780 - accuracy: 0.8972\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.2650 - accuracy: 0.9017\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.2556 - accuracy: 0.9052\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 35s 18ms/step - loss: 0.2463 - accuracy: 0.9079\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.2384 - accuracy: 0.9103\n",
      "313/313 - 1s - loss: 0.3414 - accuracy: 0.8806 - 1s/epoch - 4ms/step\n",
      "1875/1875 - 8s - loss: 0.2260 - accuracy: 0.9151 - 8s/epoch - 4ms/step\n",
      "\n",
      "Test and train accuracy: 0.8805999755859375 0.9150833487510681\n",
      "Initializer: truncated_normal\n"
     ]
    }
   ],
   "source": [
    "for initializer in relu_initializers:\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "    print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "    print('Initializer:', initializer)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:40:06.381664169Z",
     "start_time": "2023-09-24T17:14:27.676450688Z"
    }
   },
   "id": "8955e0f6e9a0a1f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Results of the network using 'he_normal' weight initializer\n",
    "Test accuracy: 0.879800021648407\n",
    "Train accuracy: 0.9140666723251343\n",
    "\n",
    "Results of the network using 'random_normal' weight initializer\n",
    "Test accuracy: 0.878000020980835\n",
    "Train accuracy: 0.9124333262443542\n",
    "\n",
    "Results of the network using 'random_uniform' weight initializer\n",
    "Test accuracy: 0.8781999945640564\n",
    "Train accuracy: 0.9146999716758728\n",
    "\n",
    "Results of the network using 'truncated_normal' weight initializer\n",
    "Test accuracy: 0.8805999755859375\n",
    "Train accuracy: 0.9150833487510681"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ab4e1936a2b7ed4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "No obvious difference between results of 'he_normal', 'random_normal', 'random_uniform', 'truncated_normal' weight initializers.\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "The choice of weight initialization method in a neural network can have a significant impact on the training and performance of the model. Each weight initialization method initializes the neural network weights differently, influencing the convergence speed and final results. Here's an overview of the impact of some commonly used weight initializers:\n",
    "\n",
    "1. **'he_normal' (He Normal Initialization)**:\n",
    "\n",
    "   - **Impact**: He Normal Initialization initializes weights with a mean of 0 and a standard deviation calculated based on the number of input units in the layer. It is specifically designed for ReLU activation functions.\n",
    "\n",
    "   - **Result**: He Normal Initialization can accelerate convergence and help prevent vanishing gradients when using ReLU activations. It is a popular choice for deep networks and is known for its effectiveness.\n",
    "\n",
    "2. **'random_normal' (Random Normal Initialization)**:\n",
    "\n",
    "   - **Impact**: Random Normal Initialization initializes weights with random values sampled from a normal distribution with a specified mean and standard deviation.\n",
    "\n",
    "   - **Result**: This initialization provides a starting point for training and allows the network to learn weight values during training. However, the effectiveness can depend on the choice of mean and standard deviation.\n",
    "\n",
    "3. **'random_uniform' (Random Uniform Initialization)**:\n",
    "\n",
    "   - **Impact**: Random Uniform Initialization initializes weights with random values sampled from a uniform distribution within a specified range.\n",
    "\n",
    "   - **Result**: Like 'random_normal,' this initialization provides an initial range of weights for training. It can be useful, but it may require careful tuning of the range to ensure effective training.\n",
    "\n",
    "4. **'truncated_normal' (Truncated Normal Initialization)**:\n",
    "\n",
    "   - **Impact**: Truncated Normal Initialization is similar to 'random_normal' but ensures that weights are sampled from a normal distribution with values within a specified range, effectively truncating any weights outside that range.\n",
    "\n",
    "   - **Result**: It can be useful for preventing very large or very small weight values, which can be beneficial for network stability.\n",
    "\n",
    "In summary:\n",
    "\n",
    "- **'he_normal'** is often a good choice when using ReLU activations, particularly in deep networks. It helps mitigate issues like vanishing gradients and can lead to faster convergence.\n",
    "\n",
    "- **'random_normal,' 'random_uniform,'** and **'truncated_normal'** provide randomness in weight initialization, allowing the network to adapt and learn weight values during training. However, their effectiveness may require careful tuning of parameters such as the mean, standard deviation, or range."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5151655d8426641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use L1 and L2 regulizations and compare results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6d54d4b78c78349"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "regularizers = [l1(0.01), l1(0.1), l1(1), l2(0.01), l2(0.1), l2(1)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T17:52:11.730519241Z",
     "start_time": "2023-09-24T17:52:11.668266701Z"
    }
   },
   "id": "b4cd738beb84fff1"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 2.2552 - accuracy: 0.7111\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 1.2192 - accuracy: 0.7553\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 1.1523 - accuracy: 0.7653\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 1.1220 - accuracy: 0.7691\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 1.0958 - accuracy: 0.7723\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.0853 - accuracy: 0.7754\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 1.0714 - accuracy: 0.7770\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.0599 - accuracy: 0.7786\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.0579 - accuracy: 0.7796\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 1.0466 - accuracy: 0.7831\n",
      "313/313 - 3s - loss: 1.0654 - accuracy: 0.7720 - 3s/epoch - 9ms/step\n",
      "1875/1875 - 13s - loss: 1.0473 - accuracy: 0.7796 - 13s/epoch - 7ms/step\n",
      "\n",
      "Test and train accuracy: 0.7720000147819519 0.7795666456222534\n",
      "Regulizer: <keras.regularizers.L1 object at 0x7ff0579b5e40>\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 9.6365 - accuracy: 0.3027\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 3.1020 - accuracy: 0.4686\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 2.9682 - accuracy: 0.5367\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 2.8272 - accuracy: 0.5903\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 2.7599 - accuracy: 0.6030\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 2.7163 - accuracy: 0.6134\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 2.6864 - accuracy: 0.6183\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 2.6603 - accuracy: 0.6237\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 2.6432 - accuracy: 0.6274\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 2.6251 - accuracy: 0.6328\n",
      "313/313 - 2s - loss: 2.5792 - accuracy: 0.6478 - 2s/epoch - 5ms/step\n",
      "1875/1875 - 14s - loss: 2.5759 - accuracy: 0.6492 - 14s/epoch - 7ms/step\n",
      "\n",
      "Test and train accuracy: 0.6478000283241272 0.6492000222206116\n",
      "Regulizer: <keras.regularizers.L1 object at 0x7ff0579b4700>\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 41s 21ms/step - loss: 75.2466 - accuracy: 0.1034\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 13.9028 - accuracy: 0.1016\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 13.7548 - accuracy: 0.1019\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 13.6909 - accuracy: 0.1044\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 13.6838 - accuracy: 0.1115\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 13.6879 - accuracy: 0.1237\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 13.6855 - accuracy: 0.1526\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 13.6898 - accuracy: 0.1704\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 37s 19ms/step - loss: 13.6983 - accuracy: 0.1837\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 13.6955 - accuracy: 0.1969\n",
      "313/313 - 2s - loss: 13.6669 - accuracy: 0.1747 - 2s/epoch - 6ms/step\n",
      "1875/1875 - 11s - loss: 13.6666 - accuracy: 0.1758 - 11s/epoch - 6ms/step\n",
      "\n",
      "Test and train accuracy: 0.17470000684261322 0.17581667006015778\n",
      "Regulizer: <keras.regularizers.L1 object at 0x7ff0579b7df0>\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 43s 22ms/step - loss: 0.8716 - accuracy: 0.7941\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 42s 22ms/step - loss: 0.6488 - accuracy: 0.8126\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 40s 22ms/step - loss: 0.6265 - accuracy: 0.8170\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.6074 - accuracy: 0.8223\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.5872 - accuracy: 0.8281\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.5778 - accuracy: 0.8295\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.5677 - accuracy: 0.8335\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.5559 - accuracy: 0.8369\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.5504 - accuracy: 0.8369\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.5459 - accuracy: 0.8400\n",
      "313/313 - 2s - loss: 0.5633 - accuracy: 0.8295 - 2s/epoch - 7ms/step\n",
      "1875/1875 - 12s - loss: 0.5332 - accuracy: 0.8430 - 12s/epoch - 7ms/step\n",
      "\n",
      "Test and train accuracy: 0.8295000195503235 0.8430333137512207\n",
      "Regulizer: <keras.regularizers.L2 object at 0x7ff0579b7100>\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 40s 21ms/step - loss: 1.3170 - accuracy: 0.7379\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.8872 - accuracy: 0.7513\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 37s 19ms/step - loss: 0.8401 - accuracy: 0.7609\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.8264 - accuracy: 0.7646\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.8067 - accuracy: 0.7713\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.7926 - accuracy: 0.7728\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 0.7790 - accuracy: 0.7774\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.7732 - accuracy: 0.7784\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.7599 - accuracy: 0.7842\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.7486 - accuracy: 0.7868\n",
      "313/313 - 2s - loss: 0.7673 - accuracy: 0.7785 - 2s/epoch - 7ms/step\n",
      "1875/1875 - 14s - loss: 0.7406 - accuracy: 0.7925 - 14s/epoch - 8ms/step\n",
      "\n",
      "Test and train accuracy: 0.7785000205039978 0.7924833297729492\n",
      "Regulizer: <keras.regularizers.L2 object at 0x7ff0579b72e0>\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 41s 21ms/step - loss: 4.0565 - accuracy: 0.6452\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 1.1910 - accuracy: 0.6739\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 1.1643 - accuracy: 0.6781\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.1209 - accuracy: 0.6902\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.1041 - accuracy: 0.6928\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 1.0907 - accuracy: 0.6992\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 35s 19ms/step - loss: 1.0624 - accuracy: 0.7057\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.0536 - accuracy: 0.7107\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 1.0361 - accuracy: 0.7139\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 1.0326 - accuracy: 0.7150\n",
      "313/313 - 2s - loss: 1.2321 - accuracy: 0.6452 - 2s/epoch - 6ms/step\n",
      "1875/1875 - 12s - loss: 1.2174 - accuracy: 0.6491 - 12s/epoch - 7ms/step\n",
      "\n",
      "Test and train accuracy: 0.6452000141143799 0.6490833163261414\n",
      "Regulizer: <keras.regularizers.L2 object at 0x7ff0579b7130>\n"
     ]
    }
   ],
   "source": [
    "for regulizer in regularizers:\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        keras.layers.Dense(128, activation='relu', kernel_regularizer=regulizer),\n",
    "        keras.layers.Dense(10)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_images, train_labels, epochs=10)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "    train_loss, train_acc = model.evaluate(train_images,  train_labels, verbose=2)\n",
    "    print('\\nTest and train accuracy:', test_acc, train_acc)\n",
    "    print('Regulizer:', regulizer)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-24T18:34:32.728288175Z",
     "start_time": "2023-09-24T17:52:11.668545748Z"
    }
   },
   "id": "f7bb1006543529be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Simple model results\n",
    "Test accuracy: 0.8819000124931335\n",
    "Train accuracy: 0.9170666933059692\n",
    "\n",
    "Network results with l1(0.01)\n",
    "Test accuracy: 0.7720000147819519\n",
    "Train accuracy: 0.7795666456222534\n",
    "\n",
    "Network results with l1(0.1)\n",
    "Test accuracy: 0.6478000283241272\n",
    "Train accuracy: 0.6492000222206116\n",
    "\n",
    "Network results with l1(1)\n",
    "Test accuracy: 0.17470000684261322\n",
    "Train accuracy: 0.17581667006015778\n",
    "\n",
    "Network results with l2(0.01)\n",
    "Test accuracy: 0.8295000195503235 \n",
    "Train accuracy: 0.8430333137512207\n",
    "\n",
    "Network results with l2(0.1)\n",
    "Test accuracy: 0.7785000205039978\n",
    "Train accuracy: 0.7924833297729492\n",
    " \n",
    "Network results with l2(1)\n",
    "Test accuracy: 0.6452000141143799\n",
    "Train accuracy: 0.6490833163261414"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f15584873322ca8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Conclusion**\n",
    "L1 and L2 regulizations definitely makes a network more stable.\n",
    "But results both on train and test sets got worse.\n",
    "\n",
    "<b style='color:red;'>FROM CHATGPT</b>\n",
    "L1 and L2 regularization are techniques used to prevent overfitting and improve the generalization performance of neural networks by adding penalty terms to the loss function. These penalty terms encourage the model to have smaller weights. Here's an overview of the impact of L1 and L2 regularization and the impact of their values:\n",
    "\n",
    "1. **L1 Regularization (Lasso Regularization)**:\n",
    "\n",
    "   - **Impact**: L1 regularization adds a penalty term to the loss function that is proportional to the absolute values of the weights. It encourages sparsity in the weight vectors, effectively driving some weights to zero.\n",
    "\n",
    "   - **Result**: The impact of L1 regularization is that it can lead to feature selection, where some weights are set to exactly zero, effectively removing certain features from consideration. This can simplify the model and make it more interpretable.\n",
    "\n",
    "   - **L1 Strength (Lambda)**: The strength of L1 regularization is controlled by a hyperparameter lambda (λ). Larger values of λ result in more aggressive weight shrinking and sparsity. Tuning λ is crucial, as it determines the trade-off between model complexity and regularization.\n",
    "\n",
    "2. **L2 Regularization (Ridge Regularization)**:\n",
    "\n",
    "   - **Impact**: L2 regularization adds a penalty term to the loss function that is proportional to the square of the weights. It discourages large weights but doesn't drive weights to exactly zero.\n",
    "\n",
    "   - **Result**: The impact of L2 regularization is that it encourages the model to distribute the weight values more evenly across all features, preventing the dominance of a few features. This often results in smoother loss landscapes and can be more forgiving to individual outliers.\n",
    "\n",
    "   - **L2 Strength (Lambda)**: Similar to L1, the strength of L2 regularization is controlled by a hyperparameter λ. Larger values of λ lead to stronger regularization.\n",
    "\n",
    "Key takeaways regarding the impact of L1 and L2 regularization:\n",
    "\n",
    "- **Overfitting Prevention**: Both L1 and L2 regularization help prevent overfitting by controlling the complexity of the model.\n",
    "\n",
    "- **Feature Selection**: L1 regularization can lead to feature selection by driving some weights to zero, making it useful for feature selection and model interpretability.\n",
    "\n",
    "- **L2 Norm vs. L1 Norm**: L2 regularization encourages all weight values to be small, while L1 regularization encourages some weights to be exactly zero.\n",
    "\n",
    "- **Hyperparameter Tuning**: The choice of the regularization strength (λ) is critical and depends on the specific problem and dataset. It is typically determined through cross-validation or other hyperparameter tuning techniques."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc525b19982f557"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f8f87f142ac7a024"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
